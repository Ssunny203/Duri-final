"""
RAG í‰ê°€ ì‹œìŠ¤í…œ - ê°„ì†Œí™” ë²„ì „ (ì‚¬ì‹¤ í™•ì¸í˜• íŠ¹í™”)
"""

import os
import time
import re
from typing import List, Dict, Any, Tuple
from datetime import datetime
from dotenv import load_dotenv

from response_formatter_v4 import StudentFriendlyFormatter

load_dotenv()


class SimplifiedRAGEvaluator:
    """ê°„ì†Œí™”ëœ ì‚¬ì‹¤ í™•ì¸í˜• RAG í‰ê°€ê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        print("ğŸ”§ í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        self.formatter = StudentFriendlyFormatter()
        
        # ì‚¬ì‹¤ í™•ì¸í˜• í‰ê°€ ê°€ì¤‘ì¹˜
        self.weights = {
            "retrieval": {
                "keyword_found": 0.4,      # í‚¤ì›Œë“œ ë°œê²¬ë¥ 
                "source_quality": 0.3,     # ì†ŒìŠ¤ í’ˆì§ˆ
                "information_density": 0.3  # ì •ë³´ ë°€ë„
            },
            "generation": {
                "fact_accuracy": 0.5,      # ì‚¬ì‹¤ ì •í™•ë„
                "completeness": 0.3,       # ì™„ì „ì„±
                "clarity": 0.2            # ëª…í™•ì„±
            },
            "overall": {
                "retrieval": 0.4,
                "generation": 0.5,
                "speed": 0.1
            }
        }
        
        print("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    
    def extract_keywords_from_question(self, question: str) -> List[str]:
        """ì§ˆë¬¸ì—ì„œ ìë™ìœ¼ë¡œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ì¡°ì‚¬ ì œê±°
        particles = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 
                    'ì™€', 'ê³¼', 'ì˜', 'ë¡œ', 'ì´ë€', 'ë€', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜']
        
        # ì˜ë¬¸ì‚¬ ì œê±°
        question_words = ['ëˆ„êµ¬', 'ì–¸ì œ', 'ì–´ë””', 'ë¬´ì—‡', 'ë­', 'ì™œ', 'ì–´ë–»ê²Œ', 
                         'ì–¼ë§ˆë‚˜', 'ëª‡', 'ì–´ëŠ', 'ì–´ë–¤']
        
        # ë™ì‚¬ ì–´ë¯¸ ì œê±°
        verb_endings = ['ì•¼', 'ì´ì•¼', 'ì˜ˆìš”', 'ì´ì—ìš”', 'ìŠµë‹ˆê¹Œ', 'ìŠµë‹ˆë‹¤', 
                       'ì–´ìš”', 'ì•„ìš”', 'ì£ ', 'ì§€', 'í–ˆì–´', 'ëì–´', 'ì˜€ì–´']
        
        words = question.split()
        keywords = []
        
        for word in words:
            # ë¬¼ìŒí‘œ ì œê±°
            word = word.replace('?', '')
            
            # ì˜ë¬¸ì‚¬ëŠ” ì œì™¸
            if word in question_words:
                continue
            
            # ì¡°ì‚¬ ì œê±°
            for particle in particles:
                if word.endswith(particle) and len(word) > len(particle):
                    word = word[:-len(particle)]
                    break
            
            # ë™ì‚¬ ì–´ë¯¸ ì œê±°
            for ending in verb_endings:
                if word.endswith(ending) and len(word) > len(ending):
                    word = word[:-len(ending)]
                    break
            
            # 2ê¸€ì ì´ìƒì¸ ëª…ì‚¬í˜• ë‹¨ì–´ë§Œ ì¶”ê°€
            if len(word) >= 2 and not any(word.endswith(v) for v in ['í•˜ë‹¤', 'ë˜ë‹¤', 'í•˜ê¸°', 'ë˜ê¸°']):
                keywords.append(word)
        
        # ì¤‘ë³µ ì œê±°í•˜ê³  ì¤‘ìš”ë„ìˆœ ì •ë ¬ (ê¸´ ë‹¨ì–´ê°€ ë” ì¤‘ìš”)
        keywords = list(dict.fromkeys(keywords))  # ìˆœì„œ ìœ ì§€í•˜ë©° ì¤‘ë³µ ì œê±°
        keywords.sort(key=len, reverse=True)
        
        return keywords[:5]  # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œ
    
    def evaluate_question(self, question: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ì§ˆë¬¸ í‰ê°€ (ê°„ì†Œí™”)"""
        # 1. í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ
        keywords = self.extract_keywords_from_question(question)
        
        # 2. RAG ì‹¤í–‰
        start_time = time.time()
        response = self.formatter.search_and_format(question)
        execution_time = time.time() - start_time
        
        # 3. ìƒì„±ëœ ë‹µë³€
        answer = response.get("main_concept", {}).get("explanation", "")
        
        # 4. í‰ê°€ ìˆ˜í–‰
        
        # Retrieval í‰ê°€
        retrieval_score = self._evaluate_retrieval(response, keywords)
        
        # Generation í‰ê°€
        generation_score = self._evaluate_generation(answer, keywords, question)
        
        # Speed í‰ê°€
        speed_score = 1.0 if execution_time < 3 else (0.7 if execution_time < 5 else 0.4)
        
        # ì¢…í•© ì ìˆ˜
        overall_score = (
            retrieval_score * self.weights["overall"]["retrieval"] +
            generation_score * self.weights["overall"]["generation"] +
            speed_score * self.weights["overall"]["speed"]
        )
        
        # ë“±ê¸‰
        grade = self._get_grade(overall_score)
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "question": question,
            "keywords": keywords,
            "answer": answer,
            "scores": {
                "retrieval": retrieval_score,
                "generation": generation_score,
                "speed": speed_score,
                "overall": overall_score
            },
            "grade": grade,
            "execution_time": execution_time,
            "confidence": response.get("confidence", "unknown")
        }
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_result(result)
        
        return result
    
    def _evaluate_retrieval(self, response: Dict[str, Any], keywords: List[str]) -> float:
        """ê²€ìƒ‰ í‰ê°€ (ì‚¬ì‹¤ í™•ì¸í˜•)"""
        scores = {}
        
        # 1. í‚¤ì›Œë“œ ë°œê²¬ë¥ 
        answer = response.get("main_concept", {}).get("explanation", "").lower()
        found_keywords = sum(1 for kw in keywords if kw.lower() in answer)
        scores["keyword_found"] = found_keywords / len(keywords) if keywords else 0.5
        
        # 2. ì†ŒìŠ¤ í’ˆì§ˆ (ì‹ ë¢°ë„ ê¸°ë°˜)
        confidence_map = {"high": 1.0, "medium": 0.7, "low": 0.4, "very_low": 0.2}
        scores["source_quality"] = confidence_map.get(response.get("confidence", "medium"), 0.5)
        
        # 3. ì •ë³´ ë°€ë„ (ì¶”ê°€ ìë£Œ ìœ ë¬´)
        info_density = 0.0
        if response.get("images"):
            info_density += 0.33
        if response.get("related_links"):
            info_density += 0.33
        if response.get("problems", {}).get("items"):
            info_density += 0.34
        scores["information_density"] = info_density
        
        # ê°€ì¤‘ í‰ê· 
        return sum(scores[k] * self.weights["retrieval"][k] for k in scores)
    
    def _evaluate_generation(self, answer: str, keywords: List[str], question: str) -> float:
        """ìƒì„± í‰ê°€ (ì‚¬ì‹¤ í™•ì¸í˜•)"""
        scores = {}
        
        # 1. ì‚¬ì‹¤ ì •í™•ë„ (í‚¤ì›Œë“œ í¬í•¨ + ìˆ«ì/ë‚ ì§œ í¬í•¨)
        keyword_score = sum(1 for kw in keywords if kw.lower() in answer.lower()) / len(keywords) if keywords else 0.5
        has_numbers = bool(re.search(r'\d+', answer))
        has_specific_info = any(term in answer for term in ['ë…„', 'ì„¸ê¸°', 'ì™•', 'ì‹œëŒ€'])
        
        scores["fact_accuracy"] = keyword_score * 0.6
        if has_numbers:
            scores["fact_accuracy"] += 0.2
        if has_specific_info:
            scores["fact_accuracy"] += 0.2
        scores["fact_accuracy"] = min(scores["fact_accuracy"], 1.0)
        
        # 2. ì™„ì „ì„± (ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ë‹µë³€)
        scores["completeness"] = 0.5
        if "ì–¸ì œ" in question and has_numbers:
            scores["completeness"] = 0.9
        elif "ëˆ„êµ¬" in question and any(name in answer for name in keywords):
            scores["completeness"] = 0.9
        elif "ë¬´ì—‡" in question or "ë­" in question:
            scores["completeness"] = 0.8 if len(answer) > 50 else 0.6
        elif len(answer) > 80:
            scores["completeness"] = 0.8
        
        # 3. ëª…í™•ì„± (ê°„ê²°í•˜ê³  ì§ì ‘ì ì¸ ë‹µë³€)
        scores["clarity"] = 1.0
        if len(answer) > 150:
            scores["clarity"] -= 0.2
        if not answer.endswith(('.', 'ìš”', 'ë‹¤', 'ì•¼')):
            scores["clarity"] -= 0.1
        if answer.count(',') > 5:  # ë„ˆë¬´ ë³µì¡í•œ ë¬¸ì¥
            scores["clarity"] -= 0.1
        scores["clarity"] = max(scores["clarity"], 0.0)
        
        # ê°€ì¤‘ í‰ê· 
        return sum(scores[k] * self.weights["generation"][k] for k in scores)
    
    def _get_grade(self, score: float) -> str:
        """ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        if score >= 0.8:
            return "A (íƒì›”)"
        elif score >= 0.7:
            return "B (ì–‘í˜¸)"
        elif score >= 0.6:
            return "C (ë³´í†µ)"
        elif score >= 0.5:
            return "D (ë¯¸í¡)"
        else:
            return "F (ë¶€ì¡±)"
    
    def _print_result(self, result: Dict[str, Any]):
        """ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*50}")
        print(f"ğŸ“Š í‰ê°€ ê²°ê³¼")
        print(f"{'='*50}")
        print(f"ğŸ”‘ í‚¤ì›Œë“œ: {', '.join(result['keywords'])}")
        print(f"ğŸ“ˆ ì ìˆ˜:")
        print(f"   ê²€ìƒ‰: {result['scores']['retrieval']:.2f}")
        print(f"   ìƒì„±: {result['scores']['generation']:.2f}")
        print(f"   ì†ë„: {result['scores']['speed']:.2f}")
        print(f"ğŸ† ì¢…í•©: {result['scores']['overall']:.2f} - {result['grade']}")
        print(f"{'='*50}")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("\nğŸ¯ RAG í‰ê°€ ì‹œìŠ¤í…œ (ê°„ì†Œí™”)")
    print("="*50)
    
    evaluator = SimplifiedRAGEvaluator()
    
    while True:
        print("\n1. ìƒˆ ì§ˆë¬¸ í‰ê°€")
        print("2. ë¹ ë¥¸ í‰ê°€ (ì—°ì†)")
        print("3. ì¢…ë£Œ")
        
        choice = input("\nì„ íƒ (1-3): ").strip()
        
        if choice == "1":
            question = input("\nì§ˆë¬¸: ").strip()
            if question:
                evaluator.evaluate_question(question)
        
        elif choice == "2":
            print("\nì—°ì† í‰ê°€ ëª¨ë“œ (ë¹ˆ ì¤„ ì…ë ¥ì‹œ ì¢…ë£Œ)")
            while True:
                question = input("\nì§ˆë¬¸: ").strip()
                if not question:
                    break
                evaluator.evaluate_question(question)
        
        elif choice == "3":
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break


if __name__ == "__main__":
    main()